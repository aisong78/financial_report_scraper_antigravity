import requests
import os
import time
import random
import sqlite3
from pathlib import Path
from datetime import datetime, timedelta

from pdf_parser import PDFParser

# å°è¯•å¯¼å…¥ç¾è‚¡ä¸‹è½½åº“ (å¦‚æœæ²¡å®‰è£…åˆ™è·³è¿‡)
try:
    from sec_edgar_downloader import Downloader as SecDownloader
    HAS_SEC = True
except ImportError:
    HAS_SEC = False

DB_PATH = Path(__file__).parent / "finance.db"

class PDFDownloader:
    def __init__(self, download_dir="downloads"):
        self.base_dir = Path(__file__).parent / download_dir
        self.base_dir.mkdir(exist_ok=True)
        self.parser = PDFParser()
        self.conn = sqlite3.connect(DB_PATH)  # æ•°æ®åº“è¿æ¥
        
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
            "Origin": "http://www.cninfo.com.cn",
            "Referer": "http://www.cninfo.com.cn/new/commonUrl/pageOfSearch?url=disclosure/list/search&lastPage=index"
        }
    
    def _record_file(self, stock_code, report_period, report_type, file_path, txt_path):
        """å°†æ–‡ä»¶ä¿¡æ¯è®°å½•åˆ°æ•°æ®åº“"""
        try:
            file_size = file_path.stat().st_size if file_path.exists() else 0
            relative_path = str(file_path.relative_to(Path(__file__).parent))
            relative_txt = str(txt_path.relative_to(Path(__file__).parent)) if txt_path else None
            
            cursor = self.conn.cursor()
            cursor.execute('''
                INSERT OR REPLACE INTO financial_reports_files
                (stock_code, report_period, report_type, file_type, file_path, txt_path, download_date, file_size, parse_status)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                stock_code,
                report_period,
                report_type,
                'PDF',
                relative_path,
                relative_txt,
                datetime.now().isoformat(),
                file_size,
                'SUCCESS' if txt_path and txt_path.exists() else 'PENDING'
            ))
            self.conn.commit()
        except Exception as e:
            print(f"  âš ï¸ è®°å½•æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {e}")
    
    def __del__(self):
        """ææ„æ—¶å…³é—­æ•°æ®åº“è¿æ¥"""
        if hasattr(self, 'conn'):
            self.conn.close()


    def _download_cninfo(self, stock_code, stock_type, save_dir, lookback_days):
        # ... (å‰é¢çš„ä»£ç ä¸å˜) ...
        
        # 3. åˆ†é¡µä¸‹è½½
        while True:
            try:
                # ... (è¯·æ±‚ä»£ç ä¸å˜) ...
                
                for ann in announcements:
                    # ... (æ–‡ä»¶åå¤„ç†ä¸å˜) ...
                    
                    # ä¸‹è½½
                    pdf_url = "http://static.cninfo.com.cn/" + ann['adjunctUrl']
                    
                    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æ‰ä¸‹è½½
                    if not file_path.exists():
                        print(f"  â¬‡ï¸ ä¸‹è½½: {title}")
                        r = requests.get(pdf_url, stream=True)
                        with open(file_path, 'wb') as f:
                            for chunk in r.iter_content(8192):
                                f.write(chunk)
                        time.sleep(0.5)
                    else:
                        print(f"  è·³è¿‡: {title}")

                    # --- é›†æˆè§£æé€»è¾‘ ---
                    # æ— è®ºæ˜¯å¦æ–°ä¸‹è½½ï¼Œéƒ½æ£€æŸ¥ä¸€ä¸‹æœ‰æ²¡æœ‰ TXTï¼Œæ²¡æœ‰å°±è§£æ
                    self.parser.parse_pdf(file_path)
                
                if not data.get('hasMore'):
                    break
                params['pageNum'] += 1
                
            except Exception as e:
                print(f"ä¸‹è½½å‡ºé”™: {e}")
                break
        
        print(f"âœ… {stock_code} ä¸‹è½½ä¸è§£æå®Œæˆï¼")

    def _download_sec(self, ticker, save_dir, lookback_days):
        # ... (ç¾è‚¡ä¸‹è½½é€»è¾‘ä¸å˜ï¼Œç¾è‚¡æœ¬èº«å°±æ˜¯ HTMLï¼Œæš‚æ—¶ä¸éœ€è¦ PDF è§£æ) ...
        # ä½†å¦‚æœæœªæ¥éœ€è¦æŠŠ HTML è½¬ TXTï¼Œä¹Ÿå¯ä»¥åœ¨è¿™é‡ŒåŠ é€»è¾‘
        pass


    def _get_stock_type(self, code):
        if code.isdigit():
            if len(code) == 6: return 'A'
            elif len(code) == 5: return 'HK'
        elif code.isalpha():
            return 'US'
        return 'UNKNOWN'

    def _get_cninfo_org_id(self, stock_code):
        """è·å–å·¨æ½®èµ„è®¯ orgId"""
        url = "http://www.cninfo.com.cn/new/information/topSearch/query"
        try:
            res = requests.post(url, data={"keyWord": stock_code}, headers=self.headers)
            if res.status_code == 200:
                data = res.json()
                for item in data:
                    if item['code'] == stock_code:
                        return item['orgId']
        except Exception as e:
            print(f"è·å– orgId å¤±è´¥: {e}")
        return None

    def download(self, stock_code, lookback_days=365*3):
        """
        é€šç”¨ä¸‹è½½å…¥å£
        """
        stock_type = self._get_stock_type(stock_code)
        save_dir = self.base_dir / stock_code
        save_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½ {stock_code} ({stock_type}) çš„è´¢æŠ¥...")
        
        if stock_type in ['A', 'HK']:
            self._download_cninfo(stock_code, stock_type, save_dir, lookback_days)
        elif stock_type == 'US':
            if HAS_SEC:
                self._download_sec(stock_code, save_dir, lookback_days)
            else:
                print("âŒ æœªå®‰è£… sec-edgar-downloaderï¼Œæ— æ³•ä¸‹è½½ç¾è‚¡è´¢æŠ¥ã€‚è¯·è¿è¡Œ: pip install sec-edgar-downloader")
        else:
            print(f"âŒ æœªçŸ¥è‚¡ç¥¨ç±»å‹: {stock_code}")

    def _download_cninfo(self, stock_code, stock_type, save_dir, lookback_days):
        url = "http://www.cninfo.com.cn/new/hisAnnouncement/query"
        
        # 1. è·å– orgId
        org_id = self._get_cninfo_org_id(stock_code)
        stock_param = f"{stock_code},{org_id}" if org_id else stock_code
        
        # 2. æ„é€ å‚æ•°
        end_date = datetime.now()
        start_date = end_date - timedelta(days=lookback_days)
        
        params = {
            "pageNum": 1,
            "pageSize": 30,
            "tabName": "fulltext",
            "stock": stock_param,
            "seDate": f"{start_date.strftime('%Y-%m-%d')}~{end_date.strftime('%Y-%m-%d')}",
            "isHLtitle": "true"
        }
        
        if stock_type == 'A':
            params['column'] = 'sse' if stock_code.startswith('6') else 'szse'
            params['category'] = "category_ndbg_szsh;category_bndbg_szsh;category_yjdbg_szsh;category_sjdbg_szsh"
        else:
            params['column'] = 'hke'
            params['category'] = "category_ndbg_hkhk;category_bndbg_hkhk"

        # 3. åˆ†é¡µä¸‹è½½
        while True:
            try:
                res = requests.post(url, data=params, headers=self.headers)
                data = res.json()
                announcements = data.get('announcements')
                
                if not announcements:
                    break
                    
                for ann in announcements:
                    title = ann['announcementTitle'].replace("<em>", "").replace("</em>", "")
                    
                    # è¿‡æ»¤æ‘˜è¦
                    if "æ‘˜è¦" in title or "å–æ¶ˆ" in title: 
                        continue
                    
                    # æ„é€ æ–‡ä»¶å
                    file_name = f"{title}.pdf"
                    file_path = save_dir / file_name.replace("/", "_")
                    
                    # ä¸‹è½½ PDF
                    if not file_path.exists():
                        pdf_url = "http://static.cninfo.com.cn/" + ann['adjunctUrl']
                        print(f"  â¬‡ï¸ ä¸‹è½½: {title}")
                        
                        r = requests.get(pdf_url, stream=True)
                        with open(file_path, 'wb') as f:
                            for chunk in r.iter_content(8192):
                                f.write(chunk)
                        time.sleep(0.5)
                    else:
                        print(f"  è·³è¿‡: {title}")
                    
                    # è§£æ PDF ä¸º TXT
                    txt_path = self.parser.parse_pdf(file_path)
                    
                    # ä»æ ‡é¢˜ä¸­æå– report_period (ç®€å•å¤„ç†)
                    # æ ‡é¢˜æ ¼å¼: "2023å¹´å¹´åº¦æŠ¥å‘Š" -> report_period: "2023-12-31"
                    report_period = self._extract_period_from_title(title)
                    report_type = self._extract_type_from_title(title)
                    
                    # è®°å½•æ–‡ä»¶ä¿¡æ¯åˆ°æ•°æ®åº“
                    if report_period and report_type:
                        self._record_file(stock_code, report_period, report_type, file_path, txt_path)
                
                if not data.get('hasMore'):
                    break
                params['pageNum'] += 1
                
            except Exception as e:
                print(f"ä¸‹è½½å‡ºé”™: {e}")
                break
        
        print(f"âœ… {stock_code} ä¸‹è½½ä¸è§£æå®Œæˆï¼")
    
    def _extract_period_from_title(self, title):
        """ä»æ ‡é¢˜æå–æŠ¥å‘ŠæœŸ"""
        import re
        # åŒ¹é…å¹´ä»½
        year_match = re.search(r'(\d{4})å¹´', title)
        if not year_match:
            return None
        year = year_match.group(1)
        
        # åˆ¤æ–­æŠ¥å‘Šç±»å‹ï¼ˆä¼˜å…ˆåˆ¤æ–­å­£åº¦/åŠå¹´ï¼Œé¿å…è¢«"å¹´åº¦"è¯¯åˆ¤ï¼‰
        if 'ç¬¬ä¸‰å­£åº¦' in title or 'ä¸‰å­£æŠ¥' in title:
            return f"{year}-09-30"
        elif 'ç¬¬ä¸€å­£åº¦' in title or 'ä¸€å­£æŠ¥' in title:
            return f"{year}-03-31"
        elif 'åŠå¹´' in title or 'ä¸­æŠ¥' in title:
            return f"{year}-06-30"
        elif 'å¹´åº¦æŠ¥å‘Š' in title or 'å¹´æŠ¥' in title:
            return f"{year}-12-31"
        return None
    
    def _extract_type_from_title(self, title):
        """ä»æ ‡é¢˜æå–æŠ¥å‘Šç±»å‹"""
        if 'ç¬¬ä¸‰å­£åº¦' in title or 'ä¸‰å­£æŠ¥' in title:
            return 'Q3'
        elif 'ç¬¬ä¸€å­£åº¦' in title or 'ä¸€å­£æŠ¥' in title:
            return 'Q1'
        elif 'åŠå¹´' in title or 'ä¸­æŠ¥' in title:
            return 'S1'
        elif 'å¹´åº¦æŠ¥å‘Š' in title or 'å¹´æŠ¥' in title:
            return 'A'
        return 'A'  # é»˜è®¤


    def _download_sec(self, ticker, save_dir, lookback_days):
        # éœ€è¦é…ç½® email
        email = "your_email@example.com" 
        dl = SecDownloader("Antigravity", email, str(save_dir))
        
        after_date = (datetime.now() - timedelta(days=lookback_days)).strftime('%Y-%m-%d')
        
        print(f"  æ­£åœ¨ä» SEC ä¸‹è½½ 10-K/10-Q (after {after_date})...")
        try:
            dl.get("10-K", ticker, after=after_date)
            dl.get("10-Q", ticker, after=after_date)
            print(f"âœ… {ticker} ç¾è‚¡è´¢æŠ¥ä¸‹è½½å®Œæˆ (HTMLæ ¼å¼)")
        except Exception as e:
            print(f"SEC ä¸‹è½½å¤±è´¥: {e}")

if __name__ == "__main__":
    d = PDFDownloader()
    # æµ‹è¯• Aè‚¡ (æµ‹è¯• 1 å¹´æ•°æ®ï¼ŒéªŒè¯æ•°æ®åº“è®°å½•åŠŸèƒ½)
    d.download("688005", lookback_days=365)
    # æµ‹è¯• æ¸¯è‚¡ (è…¾è®¯)
    # d.download("00700", lookback_days=365)
